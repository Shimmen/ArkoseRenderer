#version 460

// i.e., screen-space subsurface scattering

#include <common.glsl>
#include <common/camera.glsl>
#include <common/gBuffer.glsl>
#include <common/namedUniforms.glsl>
#include <common/visibilityBuffer.glsl>
#include <shared/SceneData.h>

layout(set = 0, binding = 0, rgba16f) uniform writeonly image2D resultImg;
layout(set = 0, binding = 1) uniform sampler2D diffuseIrradianceTex;
layout(set = 0, binding = 2) uniform sampler2D sceneDepthTex;
layout(set = 0, binding = 3) uniform sampler2D sceneBaseColorTex;
layout(set = 0, binding = 4) uniform sampler2D sceneNormalVelocityTex;
layout(set = 0, binding = 5) uniform SamplesBlock { vec4 samples[MAX_SAMPLE_COUNT]; };
layout(set = 0, binding = 6) uniform CameraStateBlock { CameraState camera; };

DeclareCommonBindingSet_VisibilityBuffer(1)
DeclareVisibilityBufferSamplingFunctions

NAMED_UNIFORMS(constants,
    uvec2 targetSize;
    uint sampleCount;
)

vec3 calculateShapeValueForVolumeAlbedo(vec3 volumeAlbedo)
{
    // Based on https://graphics.pixar.com/library/ApproxBSSRDF/approxbssrdfslides.pdf
    // Calculate the "shape" variable for the diffusion profile, using the "searchlight configuration" (see page 42)

    vec3 A = volumeAlbedo;

    return 1.85 - A + 7.0 * pow(abs(A - 0.8), vec3(3.0));
}

vec3 burleyNormalizedDiffusion(vec3 shape, float radius)
{
    vec3 s = shape;
    float r = radius;

    return s * ((exp(-s * r) + exp(-s * r / 3.0)) / (8.0 * PI));
}

layout(local_size_x = 8, local_size_y = 8) in;
void main()
{
    ivec2 pixelCoord = ivec2(gl_GlobalInvocationID.xy);
    if (any(greaterThanEqual(pixelCoord, constants.targetSize))) {
        return;
    }

    vec4 diffuseIrradiance = texelFetch(diffuseIrradianceTex, pixelCoord, 0);

    uint drawableIdx = visbuf_fetchDrawableIdx(pixelCoord);
    if (!visbuf_isValidDrawableIdx(drawableIdx)) {
        imageStore(resultImg, pixelCoord, diffuseIrradiance);
        return;
    }

    ShaderDrawable drawable = visbuf_getInstanceFromDrawableIdx(drawableIdx);
    // TODO: Make a better way to check for this bit! Some kind of DrawKey shared header & API?
    if (((drawable.drawKey >> 12) & 0xf) != 0x2) {
        imageStore(resultImg, pixelCoord, diffuseIrradiance);
        return;
    }

#if 0
    vec3 p0, p1, p2;
    uvec3 triangle = visbuf_calculateTriangle(pixelCoord);
    visbuf_getTriangleVertexPositions(triangle, p0, p1, p2);

    // NOTE: We do want the geometric normal for aligning the SSSS sample disc with
    vec3 localNormal = normalize(cross(p1 - p0, p2 - p0));
    vec3 worldNormal = mat3(drawable.worldFromTangent) * localNormal;
#else
    // This appears to look better to my eyes, as you can otherwise see the faceting in areas of high curvature.
    // In the paper this is based on they discuss that it'd be better to use the hard, geometric normals for the
    // purpose of calculating subsurface scattering, but I'm not sure I agree now. Or perhaps I misunderstood what
    // they were trying to say. Either way, I think this is better. However, with this we don't really need it to
    // be using the visibility buffer data, except for the drawableIdx exclusion. Let keep it like this for now!
    vec3 viewNormal = decodeNormal(texelFetch(sceneNormalVelocityTex, pixelCoord, 0).rg);
    vec3 worldNormal = mat3(camera.worldFromView) * viewNormal;
#endif

    vec3 B1, B2;
    createOrthonormalBasis(worldNormal, B1, B2);
    mat3 tangentBasisMatrix = mat3(B1, B2, worldNormal);

    // TODO: OR we calculate it from the visibility buffer? We just need to get the barycords & multiply with the world matrix.. so which is quicker..?
    float sceneDepthNonLinear = texelFetch(sceneDepthTex, pixelCoord, 0).r;
    vec3 pixelViewPos = unprojectPixelCoordAndDepthToViewSpace(pixelCoord, sceneDepthNonLinear, camera);
    vec3 pixelWorldPos = (camera.worldFromView * vec4(pixelViewPos, 1.0)).xyz;

    // TODO: Precalculate and put in CameraState?!
    mat4 projectionFromWorld = camera.projectionFromView * camera.viewFromWorld;

    // For all materials with subsurface scattering base color is albedo (at least for common materials)
    // Let's use the surface albedo as the volume albedo, i.e. inside skin color is the same as surface.
    vec3 volumeAlbedo = texelFetch(sceneBaseColorTex, pixelCoord, 0).rgb;
    vec3 shapeParam = calculateShapeValueForVolumeAlbedo(volumeAlbedo);

    vec3 totalIrradiance = vec3(0.0);
    vec3 totalWeight = vec3(0.0);

    for (uint idx = 0; idx < constants.sampleCount; ++idx) {

        vec4 pixelSample = samples[idx];
        const vec2 sampleCoord = pixelSample.xy;
        const float radius = pixelSample.z; // could also be calculated but easier to just pack here
        const float rcpPdf = pixelSample.w;

        // TODO: Rotate kernel over time / per frame! Important if we want to get lower sample counts.
        // NOTE: Converting sample coords from mm to m as our world space position is needed in m.
        vec3 sampleWorldPos = pixelWorldPos + (tangentBasisMatrix * vec3(sampleCoord * 0.001, 0.0));

        vec4 sampleProjPos = projectionFromWorld * vec4(sampleWorldPos, 1.0);
        vec2 sampleUv = clamp((sampleProjPos.xy / sampleProjPos.w).xy * vec2(0.5) + vec2(0.5), vec2(0.0), vec2(1.0));

        // Check if it's the same mesh (only blend within the same mesh)
        ivec2 samplePixelCoord = ivec2(clamp(sampleUv, 0.0, 0.9999) * vec2(constants.targetSize));
        uint sampleDrawableIdx = visbuf_fetchDrawableIdx(samplePixelCoord);

        if (sampleDrawableIdx == drawableIdx) {
            vec3 sampleDiffuseIrradiance = texture(diffuseIrradianceTex, sampleUv).rgb;
            vec3 weight = burleyNormalizedDiffusion(shapeParam, radius) * rcpPdf;

            totalIrradiance += sampleDiffuseIrradiance * weight;
            totalWeight += weight;
        }
    }

    totalIrradiance /= max(totalWeight, 1e-4);
    totalIrradiance = max(totalIrradiance, vec3(0.0));

    vec4 resultPixel = vec4(totalIrradiance, 1.0);
    imageStore(resultImg, pixelCoord, resultPixel);
}
