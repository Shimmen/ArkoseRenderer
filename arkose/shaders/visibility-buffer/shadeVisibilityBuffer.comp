#version 460

#include <common.glsl>
#include <common/brdf.glsl>
#include <common/lighting.glsl>
#include <common/material.glsl>
#include <common/namedUniforms.glsl>
#include <common/octahedral.glsl>
#include <common/visibilityBuffer.glsl>
#include <forward/forwardCommon.glsl>
#include <visibility-buffer/visibilityBufferCommon.glsl>
#include <shared/CameraState.h>
#include <shared/LightData.h>
#include <shared/SceneData.h>

layout(set = 0, binding = 0) uniform CameraStateBlock { CameraState camera; };

layout(set = 1, binding = 0, rgba16f) uniform restrict writeonly image2D sceneColorImg;
layout(set = 1, binding = 1, rgba16f) uniform restrict writeonly image2D diffuseIrradianceImg;
layout(set = 1, binding = 2, rgba16f) uniform restrict writeonly image2D normalVelocityImg;
layout(set = 1, binding = 3, rgba8) uniform restrict writeonly image2D materialPropertyImg;
layout(set = 1, binding = 4, rgba8) uniform restrict writeonly image2D baseColorImg;

DeclareCommonBindingSet_VisibilityBuffer(2)
DeclareCommonBindingSet_Material(3)
DeclareCommonBindingSet_Light(4)

layout(set = 5, binding = 0) uniform sampler2D directionalLightProjectedShadowTex;
layout(set = 5, binding = 1) uniform sampler2D sphereLightProjectedShadowTex;
layout(set = 5, binding = 2) uniform sampler2D localLightShadowMapAtlasTex;
layout(set = 5, binding = 3) buffer readonly ShadowMapViewportBlock { vec4 localLightShadowMapViewports[]; };

NAMED_UNIFORMS_STRUCT(ForwardPassConstants, constants)

DeclareVisibilityBufferSamplingFunctions

// NOTE: This is only really for debugging! In general we try to avoid permutations for very common cases (almost everything will be normal mapped in practice)
// (If we want to make normal mapping a proper permutation we would also want to exclude interpolats vTangent and vBitangentSign)
#define USE_NORMAL_MAPPING 1
// NOTE: We have to use 2-component normals when using BC5 compressed normal maps, but we always *can* use it, which is nice since we avoid permutations.
// In practice we will loose some level of precision by doing the reconstruction though, so the old path is left for A/B comparison purposes.
#define USE_2COMPONENT_NORMALS 1

// If we have subsurface scattering in the pipeline we need to keep diffuse irradiance separate so we can "blur" that for those pixels before we apply
// it to the scene color. Use the diffuse irradiance (i.e. the irradiance that enters the surface) for that.
#define INCLUDE_DIFFUSE_IN_SCENE_COLOR() (0)

vec3 evaluateDirectionalLight(DirectionalLightData light, bool hasShadow, uvec2 pixelCoord, vec3 V, vec3 N, vec3 baseColor, float roughness, float metallic, inout vec3 outDiffuseIrradiance)
{
    vec3 L = -normalize(light.viewSpaceDirection.xyz);

    vec2 sampleTexCoords = (vec2(pixelCoord) + vec2(0.5)) * constants.invTargetSize;
    float shadowFactor = hasShadow ? textureLod(directionalLightProjectedShadowTex, sampleTexCoords, 0).r : 1.0;

    vec3 F;
    vec3 specular = specularBRDF(L, V, N, baseColor, roughness, metallic, F);

    float LdotN = max(dot(L, N), 0.0);
    vec3 irradiance = light.color * LdotN * shadowFactor;

    outDiffuseIrradiance += (1.0 - F) * vec3(1.0 - metallic) * irradiance;

#if INCLUDE_DIFFUSE_IN_SCENE_COLOR()
    vec3 diffuse = outDiffuseIrradiance * baseColor * diffuseBRDF();
    return (irradiance * specular) + diffuse;
#else
    return irradiance * specular;
#endif
}

float evaluateLocalLightShadow(uint shadowIdx, mat4 lightProjectionFromView, vec3 viewSpacePos)
{
    vec4 shadowViewport = localLightShadowMapViewports[shadowIdx];

    // No shadow for this light
    if (lengthSquared(shadowViewport) < 1e-4) {
        return 1.0;
    }

    vec4 posInShadowMap = lightProjectionFromView * vec4(viewSpacePos, 1.0);
    posInShadowMap.xyz /= posInShadowMap.w;

    vec2 shadowMapUv = (posInShadowMap.xy * 0.5 + 0.5); // uv in the whole atlas
    shadowMapUv *= shadowViewport.zw; // scale to the appropriate viewport size
    shadowMapUv += shadowViewport.xy; // offset to the first pixel of the viewport

    float mapDepth = textureLod(localLightShadowMapAtlasTex, shadowMapUv, 0).x;
    return (mapDepth < posInShadowMap.z) ? 0.0 : 1.0;
}

vec3 evaluateSphereLight(SphereLightData light, bool hasShadow, uvec2 pixelCoord, vec3 viewSpacePos, vec3 V, vec3 N, vec3 geometricNormal, vec3 baseColor, float roughness, float metallic, inout vec3 outDiffuseIrradiance)
{
    // TODO: Support multiple sphere lights with shadows!
    vec2 sampleTexCoords = (vec2(pixelCoord) + vec2(0.5)) * constants.invTargetSize;
    float shadowFactor = hasShadow ? textureLod(sphereLightProjectedShadowTex, sampleTexCoords, 0).r : 1.0;

    vec3 toLight = light.viewSpacePosition.xyz - viewSpacePos;
    vec3 L = normalize(toLight);

    // If the light source is behind the geometric normal of the surface consider it in shadow,
    // even if a normal map could make the surface seem to be able to pick up light from the light.
    if (dot(normalize(geometricNormal), L) < 0.0) {
        shadowFactor = 0.0;
    }

    float dist = length(toLight);
    float distanceAttenuation = calculateLightDistanceAttenuation(dist, light.lightSourceRadius, light.lightRadius);

    vec3 F;
    vec3 specular = specularBRDF(L, V, N, baseColor, roughness, metallic, F);

    float LdotN = max(dot(L, N), 0.0);
    vec3 irradiance = light.color * LdotN * shadowFactor * distanceAttenuation;

    outDiffuseIrradiance += (1.0 - F) * vec3(1.0 - metallic) * irradiance;

#if INCLUDE_DIFFUSE_IN_SCENE_COLOR()
    vec3 diffuse = outDiffuseIrradiance * baseColor * diffuseBRDF();
    return (irradiance * specular) + diffuse;
#else
    return irradiance * specular;
#endif
}

vec3 evaluateSpotLight(SpotLightData light, uint shadowIdx, vec3 viewSpacePos, vec3 V, vec3 N, vec3 baseColor, float roughness, float metallic, inout vec3 outDiffuseIrradiance)
{
    vec3 L = -normalize(light.viewSpaceDirection.xyz);

    float shadowFactor = evaluateLocalLightShadow(shadowIdx, light.lightProjectionFromView, viewSpacePos);

    vec3 toLight = light.viewSpacePosition.xyz - viewSpacePos;
    float dist = length(toLight);
    float distanceAttenuation = 1.0 / square(dist);

    float cosConeAngle = dot(L, toLight / dist);
    float iesValue = evaluateIESLookupTable(material_getTexture(light.iesProfileIndex), light.outerConeHalfAngle, cosConeAngle);

    vec3 F;
    vec3 specular = specularBRDF(L, V, N, baseColor, roughness, metallic, F);

    float LdotN = max(dot(L, N), 0.0);
    vec3 irradiance = light.color * iesValue * LdotN * shadowFactor * distanceAttenuation;

    outDiffuseIrradiance += (1.0 - F) * vec3(1.0 - metallic) * irradiance;

#if INCLUDE_DIFFUSE_IN_SCENE_COLOR()
    vec3 diffuse = outDiffuseIrradiance * baseColor * diffuseBRDF();
    return (irradiance * specular) + diffuse;
#else
    return irradiance * specular;
#endif
}

layout(local_size_x = 8, local_size_y = 8) in;
void main()
{
    ivec2 pixelCoord = ivec2(gl_GlobalInvocationID.xy);
    ivec2 imageSize = ivec2(imageSize(sceneColorImg));

    if (any(greaterThanEqual(pixelCoord, imageSize))) {
        return;
    }

    uint drawableIdx = visbuf_fetchDrawableIdx(pixelCoord);
    if (!visbuf_isValidDrawableIdx(drawableIdx)) {
        // TODO: Consider if we should do an environment map lookup here, or just early exit?
        imageStore(sceneColorImg, pixelCoord, vec4(0.0, 0.0, 0.0, 1.0));
        return;
    }

    ShaderDrawable drawable = visbuf_getInstanceFromDrawableIdx(drawableIdx);
    ShaderMaterial material = material_getMaterial(drawable.materialIndex);

    vec3 p0, p1, p2;
    NonPositionVertex v0, v1, v2;
    uvec3 triangle = visbuf_calculateTriangle(pixelCoord);
    visbuf_getTriangleVertexPositions(triangle, p0, p1, p2);
    visbuf_getTriangleVertexNonPositionData(triangle, v0, v1, v2);

    mat4 viewFromLocal = camera.viewFromWorld * drawable.worldFromLocal;
    vec4 viewSpacePos0 = viewFromLocal * vec4(p0, 1.0);
    vec4 viewSpacePos1 = viewFromLocal * vec4(p1, 1.0);
    vec4 viewSpacePos2 = viewFromLocal * vec4(p2, 1.0);

    vec4 projectedPos0 = camera.projectionFromView * viewSpacePos0;
    vec4 projectedPos1 = camera.projectionFromView * viewSpacePos1;
    vec4 projectedPos2 = camera.projectionFromView * viewSpacePos2;

    vec2 windowSize = vec2(imageSize);
    vec2 pixelNdc = ((vec2(pixelCoord) + vec2(0.5)) / windowSize) * 2.0 - 1.0;
    BarycentricDeriv barycentrics = CalcFullBary(projectedPos0, projectedPos1, projectedPos2, pixelNdc, windowSize);

    vec3 viewSpacePos = InterpolateVec3(barycentrics, viewSpacePos0.xyz, viewSpacePos1.xyz, viewSpacePos2.xyz);
    vec3 V = -normalize(viewSpacePos);

    vec2 velocity;
    {
        vec4 currentFrameProjectedPos = InterpolateVec4(barycentrics, projectedPos0, projectedPos1, projectedPos2);

        mat4 previousFrameProjectionFromLocal = camera.previousFrameProjectionFromView * camera.previousFrameViewFromWorld * drawable.previousFrameWorldFromLocal;
        vec4 previousFrameProjectedPos = InterpolateVec4(barycentrics,
            previousFrameProjectionFromLocal * vec4(p0, 1.0),
            previousFrameProjectionFromLocal * vec4(p1, 1.0),
            previousFrameProjectionFromLocal * vec4(p2, 1.0));

        vec2 currentPos = currentFrameProjectedPos.xy / currentFrameProjectedPos.w;
        vec2 previousPos = previousFrameProjectedPos.xy / previousFrameProjectedPos.w;

        velocity = (currentPos - previousPos) * vec2(0.5, 0.5); // in uv-space
        velocity -= constants.frustumJitterCorrection;
    }

    vec3 texCoordX = InterpolateWithDerivatives(barycentrics, v0.texcoord0.x, v1.texcoord0.x, v2.texcoord0.x);
    vec3 texCoordY = InterpolateWithDerivatives(barycentrics, v0.texcoord0.y, v1.texcoord0.y, v2.texcoord0.y);
    vec2 texCoord = vec2(texCoordX.x, texCoordY.x);
    vec2 ddx = vec2(texCoordX.y, texCoordY.y) * constants.mipBias; // NOTE: Ensure constants.mipBias is exp2(actual mip bias) as we're working
    vec2 ddy = vec2(texCoordX.z, texCoordY.z) * constants.mipBias; // directly with gradients here (see cpp-file for additional details)

    // NOTE: Ensure we normalize before interpolating and not after, according to MikkT space (http://www.mikktspace.com/)
    mat3 viewFromTangent = mat3(camera.viewFromWorld) * mat3(drawable.worldFromTangent);
    vec3 N = InterpolateVec3(barycentrics,
        normalize(viewFromTangent * v0.normal),
        normalize(viewFromTangent * v1.normal),
        normalize(viewFromTangent * v2.normal));
    vec3 T = InterpolateVec3(barycentrics,
        normalize(viewFromTangent * v0.tangent.xyz),
        normalize(viewFromTangent * v1.tangent.xyz),
        normalize(viewFromTangent * v2.tangent.xyz));
    float bitangentSign = v0.tangent.w;

    // If we're seeing the backface of the visibility buffer triangle we're shading it must be double sided
    if (dot(V, N) < 0.0) {
        N = -N;
    }

    #if USE_NORMAL_MAPPING
        vec3 packedNormal = textureGrad(material_getTexture(material.normalMap), texCoord, ddx, ddy).rgb;
        #if USE_2COMPONENT_NORMALS
            vec3 tangentNormal = vec3(packedNormal.rg * 2.0 - 1.0, 0.0);
            tangentNormal.z = sqrt(clamp(1.0 - lengthSquared(tangentNormal.xy), 0.0, 1.0));
        #else
            vec3 tangentNormal = packedNormal * 2.0 - 1.0;
        #endif

        // Save the pre-normal-mapped normal
        // It's not strictly a geometric normal, as it's interpolated, but it's close enough for this..
        vec3 geometricNormal = N;

        // Apply normal map according to MikkT space
        vec3 B = bitangentSign * cross(N, T);
        N = normalize(tangentNormal.x * T + tangentNormal.y * B + tangentNormal.z * N);
    #else
        vec3 geometricNormal = N;
    #endif

    vec3 baseColor = material.colorTint.rgb * textureGrad(material_getTexture(material.baseColor), texCoord, ddx, ddy).rgb;
    if (!constants.withMaterialColor) {
        baseColor = vec3(1.0);
    }

    vec3 emissive = textureGrad(material_getTexture(material.emissive), texCoord, ddx, ddy).rgb;

    vec4 metallicRoughness = textureGrad(material_getTexture(material.metallicRoughness), texCoord, ddx, ddy);
    float metallic = metallicRoughness.b * material.metallicFactor;
    float roughness = metallicRoughness.g * material.roughnessFactor;

    vec3 ambient = constants.ambientAmount * baseColor;
    vec3 sceneColor = emissive + ambient;
    vec3 diffuseIrradiance = vec3(0.0);

    for (uint i = 0; i < light_getDirectionalLightCount(); ++i) {
        // We only have shadow for the 0th directional light as they are pre-projected. If needed we could quite easily support up to 4 shadowed directional light
        // by storing the projected shadow in an RGBA texture with a projected shadow per channel. However, a single dir. shadow should almost always be enough.
        bool hasShadow = i == 0;

        sceneColor += evaluateDirectionalLight(light_getDirectionalLight(i), hasShadow, pixelCoord, V, N, baseColor, roughness, metallic, diffuseIrradiance);
    }

    // TODO: Use tiles or clusters to minimize number of light evaluations!
    {
        for (uint i = 0; i < light_getSphereLightCount(); ++i) {
            bool hasShadow = i == 0; // todo: support multiple shadowed point lights!
            sceneColor += evaluateSphereLight(light_getSphereLight(i), hasShadow, pixelCoord, viewSpacePos, V, N, geometricNormal, baseColor, roughness, metallic, diffuseIrradiance);
        }

        uint shadowIdx = 0;
        for (uint i = 0; i < light_getSpotLightCount(); ++i) {
            sceneColor += evaluateSpotLight(light_getSpotLight(i), shadowIdx++, viewSpacePos, V, N, baseColor, roughness, metallic, diffuseIrradiance);
        }
    }

    imageStore(sceneColorImg, pixelCoord, vec4(sceneColor, 1.0));
    imageStore(diffuseIrradianceImg, pixelCoord, vec4(diffuseIrradiance, 1.0));

    // TODO: Eventually we probably don't want to be writing these.. this is just for the visibility buffer transition period
    imageStore(normalVelocityImg, pixelCoord, vec4(octahedralEncode(N), velocity));
    imageStore(materialPropertyImg, pixelCoord, vec4(roughness, metallic, 0.0, 0.0));
    imageStore(baseColorImg, pixelCoord, vec4(baseColor, 1.0));
}
