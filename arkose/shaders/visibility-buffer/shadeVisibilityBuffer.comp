#version 460

#extension GL_EXT_scalar_block_layout : require
#extension GL_EXT_nonuniform_qualifier : require

#include <common.glsl>
#include <common/brdf.glsl>
#include <common/lighting.glsl>
#include <common/namedUniforms.glsl>
#include <common/octahedral.glsl>
#include <forward/forwardCommon.glsl>
#include <visibility-buffer/visibilityBufferCommon.glsl>
#include <shared/CameraState.h>
#include <shared/LightData.h>
#include <shared/SceneData.h>

layout(set = 0, binding = 0) uniform CameraStateBlock { CameraState camera; };

layout(set = 1, binding = 0, rgba16f) uniform restrict writeonly image2D sceneColorImg;
layout(set = 1, binding = 1, rgba16f) uniform restrict writeonly image2D normalVelocityImg;
layout(set = 1, binding = 2, rgba8) uniform restrict writeonly image2D materialPropertyImg;
layout(set = 1, binding = 3, rgba8) uniform restrict writeonly image2D baseColorImg;

layout(set = 2, binding = 0) uniform usampler2D drawableVisibilityTex;
layout(set = 2, binding = 1) uniform usampler2D triangleVisibilityTex;
layout(set = 2, binding = 2) buffer restrict readonly InstanceBlock { ShaderDrawable instances[]; };
layout(set = 2, binding = 3) buffer restrict readonly MeshletBlock { ShaderMeshlet meshlets[]; };
layout(set = 2, binding = 4, scalar) buffer restrict readonly IndicesBlock { uint meshletIndices[]; };
layout(set = 2, binding = 5, scalar) buffer restrict readonly PositionsBlock { vec3 meshletPositions[]; };
layout(set = 2, binding = 6, scalar) buffer restrict readonly NonPositionsBlock { NonPositionVertex meshletVertices[]; };

layout(set = 3, binding = 0) buffer readonly MaterialBlock { ShaderMaterial materials[]; };
layout(set = 3, binding = 1) uniform sampler2D textures[];

layout(set = 4, binding = 0) uniform LightMetaDataBlock { LightMetaData lightMeta; };
layout(set = 4, binding = 1) buffer readonly DirLightDataBlock { DirectionalLightData directionalLights[]; };
layout(set = 4, binding = 2) buffer readonly SphereLightDataBlock { SphereLightData sphereLights[]; };
layout(set = 4, binding = 3) buffer readonly SpotLightDataBlock { SpotLightData spotLights[]; };

layout(set = 5, binding = 0) uniform sampler2D directionalLightProjectedShadowTex;
layout(set = 5, binding = 1) uniform sampler2D sphereLightProjectedShadowTex;
layout(set = 5, binding = 2) uniform sampler2D localLightShadowMapAtlasTex;
layout(set = 5, binding = 3) buffer readonly ShadowMapViewportBlock { vec4 localLightShadowMapViewports[]; };

NAMED_UNIFORMS_STRUCT(ForwardPassConstants, constants)

// NOTE: This is only really for debugging! In general we try to avoid permutations for very common cases (almost everything will be normal mapped in practice)
// (If we want to make normal mapping a proper permutation we would also want to exclude interpolats vTangent and vBitangentSign)
#define USE_NORMAL_MAPPING 1
// NOTE: We have to use 2-component normals when using BC5 compressed normal maps, but we always *can* use it, which is nice since we avoid permutations.
// In practice we will loose some level of precision by doing the reconstruction though, so the old path is left for A/B comparison purposes.
#define USE_2COMPONENT_NORMALS 1

vec3 evaluateDirectionalLight(DirectionalLightData light, bool hasShadow, uvec2 pixelCoord, vec3 V, vec3 N, vec3 baseColor, float roughness, float metallic)
{
    vec3 L = -normalize(light.viewSpaceDirection.xyz);

    vec2 sampleTexCoords = (vec2(pixelCoord) + vec2(0.5)) * constants.invTargetSize;
    float shadowFactor = hasShadow ? textureLod(directionalLightProjectedShadowTex, sampleTexCoords, 0).r : 1.0;

    vec3 brdf = evaluateBRDF(L, V, N, baseColor, roughness, metallic);
    vec3 directLight = light.color * shadowFactor;

    float LdotN = max(dot(L, N), 0.0);
    return brdf * LdotN * directLight;
}

float evaluateLocalLightShadow(uint shadowIdx, mat4 lightProjectionFromView, vec3 viewSpacePos)
{
    vec4 shadowViewport = localLightShadowMapViewports[shadowIdx];

    // No shadow for this light
    if (lengthSquared(shadowViewport) < 1e-4) {
        return 1.0;
    }

    vec4 posInShadowMap = lightProjectionFromView * vec4(viewSpacePos, 1.0);
    posInShadowMap.xyz /= posInShadowMap.w;

    vec2 shadowMapUv = (posInShadowMap.xy * 0.5 + 0.5); // uv in the whole atlas
    shadowMapUv *= shadowViewport.zw; // scale to the appropriate viewport size
    shadowMapUv += shadowViewport.xy; // offset to the first pixel of the viewport

    float mapDepth = textureLod(localLightShadowMapAtlasTex, shadowMapUv, 0).x;
    return (mapDepth < posInShadowMap.z) ? 0.0 : 1.0;
}

vec3 evaluateSphereLight(SphereLightData light, bool hasShadow, uvec2 pixelCoord, vec3 viewSpacePos, vec3 V, vec3 N, vec3 geometricNormal, vec3 baseColor, float roughness, float metallic)
{
    // TODO: Support multiple sphere lights with shadows!
    vec2 sampleTexCoords = (vec2(pixelCoord) + vec2(0.5)) * constants.invTargetSize;
    float shadowFactor = hasShadow ? textureLod(sphereLightProjectedShadowTex, sampleTexCoords, 0).r : 1.0;

    vec3 toLight = light.viewSpacePosition.xyz - viewSpacePos;
    vec3 L = normalize(toLight);

    // If the light source is behind the geometric normal of the surface consider it in shadow,
    // even if a normal map could make the surface seem to be able to pick up light from the light.
    if (dot(normalize(geometricNormal), L) < 0.0) {
        shadowFactor = 0.0;
    }

    float dist = length(toLight);
    float distanceAttenuation = calculateLightDistanceAttenuation(dist, light.lightSourceRadius, light.lightRadius);

    vec3 brdf = evaluateBRDF(L, V, N, baseColor, roughness, metallic);
    vec3 directLight = light.color * shadowFactor * distanceAttenuation;

    float LdotN = max(dot(L, N), 0.0);
    return brdf * LdotN * directLight;
}

vec3 evaluateSpotLight(SpotLightData light, uint shadowIdx, vec3 viewSpacePos, vec3 V, vec3 N, vec3 baseColor, float roughness, float metallic)
{
    vec3 L = -normalize(light.viewSpaceDirection.xyz);

    float shadowFactor = evaluateLocalLightShadow(shadowIdx, light.lightProjectionFromView, viewSpacePos);

    vec3 toLight = light.viewSpacePosition.xyz - viewSpacePos;
    float dist = length(toLight);
    float distanceAttenuation = 1.0 / square(dist);

    float cosConeAngle = dot(L, toLight / dist);
    float iesValue = evaluateIESLookupTable(textures[nonuniformEXT(light.iesProfileIndex)], light.outerConeHalfAngle, cosConeAngle);

    vec3 brdf = evaluateBRDF(L, V, N, baseColor, roughness, metallic);
    vec3 directLight = light.color * shadowFactor * distanceAttenuation * iesValue;

    float LdotN = max(dot(L, N), 0.0);
    return brdf * LdotN * directLight;
}

layout(local_size_x = 8, local_size_y = 8) in;
void main()
{
    ivec2 pixelCoord = ivec2(gl_GlobalInvocationID.xy);
    ivec2 imageSize = ivec2(imageSize(sceneColorImg));

    if (any(greaterThanEqual(pixelCoord, imageSize))) {
        return;
    }

    uint drawableIdx = uint(texelFetch(drawableVisibilityTex, pixelCoord, 0).x);
    if (drawableIdx == 0) {
        // TODO: Consider if we should do an environment map lookup here, or just early exit?
        imageStore(sceneColorImg, pixelCoord, vec4(0.0, 0.0, 0.0, 1.0));
        return;
    }

    ShaderDrawable drawable = instances[drawableIdx - 1];
    ShaderMaterial material = materials[drawable.materialIndex];

    uint triangleId = uint(texelFetch(triangleVisibilityTex, pixelCoord, 0).x);
    uint meshletTriangleIdx = triangleId & 0xff;
    uint meshletIdx = (triangleId >> 8) - 1;

    ShaderMeshlet meshlet = meshlets[meshletIdx];
    uint meshletBaseIdxLookup = meshlet.firstIndex + (3 * meshletTriangleIdx);
    uint i0 = meshletIndices[meshletBaseIdxLookup + 0];
    uint i1 = meshletIndices[meshletBaseIdxLookup + 1];
    uint i2 = meshletIndices[meshletBaseIdxLookup + 2];

    vec3 p0 = meshletPositions[i0];
    vec3 p1 = meshletPositions[i1];
    vec3 p2 = meshletPositions[i2];

    mat4 viewFromLocal = camera.viewFromWorld * drawable.worldFromLocal;
    vec4 viewSpacePos0 = viewFromLocal * vec4(p0, 1.0);
    vec4 viewSpacePos1 = viewFromLocal * vec4(p1, 1.0);
    vec4 viewSpacePos2 = viewFromLocal * vec4(p2, 1.0);

    vec4 projectedPos0 = camera.projectionFromView * viewSpacePos0;
    vec4 projectedPos1 = camera.projectionFromView * viewSpacePos1;
    vec4 projectedPos2 = camera.projectionFromView * viewSpacePos2;

    NonPositionVertex v0 = meshletVertices[i0];
    NonPositionVertex v1 = meshletVertices[i1];
    NonPositionVertex v2 = meshletVertices[i2];

    vec2 windowSize = vec2(imageSize);
    vec2 pixelNdc = ((vec2(pixelCoord) + vec2(0.5)) / windowSize) * 2.0 - 1.0;
    BarycentricDeriv barycentrics = CalcFullBary(projectedPos0, projectedPos1, projectedPos2, pixelNdc, windowSize);

    vec3 viewSpacePos = InterpolateVec3(barycentrics, viewSpacePos0.xyz, viewSpacePos1.xyz, viewSpacePos2.xyz);
    vec3 V = -normalize(viewSpacePos);

    vec2 velocity;
    {
        vec4 currentFrameProjectedPos = InterpolateVec4(barycentrics, projectedPos0, projectedPos1, projectedPos2);

        mat4 previousFrameProjectionFromLocal = camera.previousFrameProjectionFromView * camera.previousFrameViewFromWorld * drawable.previousFrameWorldFromLocal;
        vec4 previousFrameProjectedPos = InterpolateVec4(barycentrics,
            previousFrameProjectionFromLocal * vec4(p0, 1.0),
            previousFrameProjectionFromLocal * vec4(p1, 1.0),
            previousFrameProjectionFromLocal * vec4(p2, 1.0));

        vec2 currentPos = currentFrameProjectedPos.xy / currentFrameProjectedPos.w;
        vec2 previousPos = previousFrameProjectedPos.xy / previousFrameProjectedPos.w;

        velocity = (currentPos - previousPos) * vec2(0.5, 0.5); // in uv-space
        velocity -= constants.frustumJitterCorrection;
    }

    vec3 texCoordX = InterpolateWithDerivatives(barycentrics, v0.texcoord0.x, v1.texcoord0.x, v2.texcoord0.x);
    vec3 texCoordY = InterpolateWithDerivatives(barycentrics, v0.texcoord0.y, v1.texcoord0.y, v2.texcoord0.y);
    vec2 texCoord = vec2(texCoordX.x, texCoordY.x);
    vec2 ddx = vec2(texCoordX.y, texCoordY.y) * constants.mipBias; // NOTE: Ensure constants.mipBias is exp2(actual mip bias) as we're working
    vec2 ddy = vec2(texCoordX.z, texCoordY.z) * constants.mipBias; // directly with gradients here (see cpp-file for additional details)

    // NOTE: Ensure we normalize before interpolating and not after, according to MikkT space (http://www.mikktspace.com/)
    mat3 viewFromTangent = mat3(camera.viewFromWorld) * mat3(drawable.worldFromTangent);
    vec3 N = InterpolateVec3(barycentrics,
        normalize(viewFromTangent * v0.normal),
        normalize(viewFromTangent * v1.normal),
        normalize(viewFromTangent * v2.normal));
    vec3 T = InterpolateVec3(barycentrics,
        normalize(viewFromTangent * v0.tangent.xyz),
        normalize(viewFromTangent * v1.tangent.xyz),
        normalize(viewFromTangent * v2.tangent.xyz));
    float bitangentSign = v0.tangent.w;

    // If we're seeing the backface of the visibility buffer triangle we're shading it must be double sided
    if (dot(V, N) < 0.0) {
        N = -N;
    }

    #if USE_NORMAL_MAPPING
        vec3 packedNormal = textureGrad(textures[nonuniformEXT(material.normalMap)], texCoord, ddx, ddy).rgb;
        #if USE_2COMPONENT_NORMALS
            vec3 tangentNormal = vec3(packedNormal.rg * 2.0 - 1.0, 0.0);
            tangentNormal.z = sqrt(clamp(1.0 - lengthSquared(tangentNormal.xy), 0.0, 1.0));
        #else
            vec3 tangentNormal = packedNormal * 2.0 - 1.0;
        #endif

        // Save the pre-normal-mapped normal
        // It's not strictly a geometric normal, as it's interpolated, but it's close enough for this..
        vec3 geometricNormal = N;

        // Apply normal map according to MikkT space
        vec3 B = bitangentSign * cross(N, T);
        N = normalize(tangentNormal.x * T + tangentNormal.y * B + tangentNormal.z * N);
    #else
        vec3 geometricNormal = N;
    #endif

    vec3 baseColor = material.colorTint.rgb * textureGrad(textures[nonuniformEXT(material.baseColor)], texCoord, ddx, ddy).rgb;
    vec3 emissive = textureGrad(textures[nonuniformEXT(material.emissive)], texCoord, ddx, ddy).rgb;

    vec4 metallicRoughness = textureGrad(textures[nonuniformEXT(material.metallicRoughness)], texCoord, ddx, ddy);
    float metallic = metallicRoughness.b * material.metallicFactor;
    float roughness = metallicRoughness.g * material.roughnessFactor;

    vec3 ambient = constants.ambientAmount * baseColor;
    vec3 sceneColor = emissive + ambient;

    for (uint i = 0; i < lightMeta.numDirectionalLights; ++i) {
        // We only have shadow for the 0th directional light as they are pre-projected. If needed we could quite easily support up to 4 shadowed directional light
        // by storing the projected shadow in an RGBA texture with a projected shadow per channel. However, a single dir. shadow should almost always be enough.
        bool hasShadow = i == 0;

        sceneColor += evaluateDirectionalLight(directionalLights[i], hasShadow, pixelCoord, V, N, baseColor, roughness, metallic);
    }

    // TODO: Use tiles or clusters to minimize number of light evaluations!
    {
        for (uint i = 0; i < lightMeta.numSphereLights; ++i) {
            bool hasShadow = i == 0; // todo: support multiple shadowed point lights!
            sceneColor += evaluateSphereLight(sphereLights[i], hasShadow, pixelCoord, viewSpacePos, V, N, geometricNormal, baseColor, roughness, metallic);
        }

        uint shadowIdx = 0;
        for (uint i = 0; i < lightMeta.numSpotLights; ++i) {
            sceneColor += evaluateSpotLight(spotLights[i], shadowIdx++, viewSpacePos, V, N, baseColor, roughness, metallic);
        }
    }

    imageStore(sceneColorImg, pixelCoord, vec4(sceneColor, 1.0));

    // TODO: Eventually we probably don't want to be writing these.. this is just for the visibility buffer transition period
    imageStore(normalVelocityImg, pixelCoord, vec4(octahedralEncode(N), velocity));
    imageStore(materialPropertyImg, pixelCoord, vec4(roughness, metallic, 0.0, 0.0));
    imageStore(baseColorImg, pixelCoord, vec4(baseColor, 1.0));
}
